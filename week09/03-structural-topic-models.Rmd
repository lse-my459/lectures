---
title: "Structural topic models"
author: "Friedrich Geiecke"
date: "14 March 2022"
output: html_document
---

Loading packages:

```{r}
#install.packages("stm")

library("quanteda")
library("stm")
library("tidyverse")
library("stringi")
library("lubridate")
```


## 1. Text processing

In this notebook, we will illustrate functionalities of structural topic models with the help of a random sample of 10,000 public Facebook posts by members of the U.S. Congress from 2017. With this data, we will use and explore code examples from the `stm` package [vignette](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf). Loading the data:

```{r}
df <- read.csv("fb-congress-data.csv") %>% as_tibble()
df$message <- stri_unescape_unicode(df$message) # unescaping unicodes
colnames(df)
```

Processing it:

```{r}
# Keeping only two main parties and only the relevant columns
df <- df %>% filter(party %in% c("Democrat", "Republican")) %>%
  select(screen_name, date, message, gender, party) %>% drop_na()

# Creating a day of the year column
df$date <- as_datetime(df$date)
df$day_of_year <- yday(df$date)

# Transforming gender and party into factors
df$gender <- factor(df$gender)
df$party <- factor(df$party)

# Specifying a specific order of factors
# This makes Republican being label 1 and democrat being label 2
df$party <- factor(df$party, levels = c("Republican", "Democrat"))

nrow(df)
head(df)
```

Creating a corpus and document feature matrix with `quanteda`:

```{r}
posts_corpus <- corpus(df$message,
                       docvars = df[,c("screen_name", "date", "day_of_year",
                                       "gender", "party")])
```


```{r}
posts_dfm <- posts_corpus %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,
         remove_url = TRUE) %>%
  tokens_remove(stopwords("en"), padding = TRUE) %>%
  tokens_ngrams(n = 1:2) %>%
  dfm() %>%
  dfm_trim(min_termfreq = 5)
posts_dfm
```

Converting the `quanteda` object into an input for the structural topic model:

```{r}
stm_input <- convert(posts_dfm, to = "stm")
```


## 2. Fitting a topic model with topic prevalence covariates

To obtain some quantitative diagnostics, you can run a code chunk like the following, however, it can run for several hours already on a dataset of this size. You could run it after this coding sessions for a smaller set of K values to try it out.

```{r, eval = FALSE}
k_search_output <- searchK(stm_input$documents, stm_input$vocab,
                           K = c(5, 10, 25, 50, 100), data = stm_input$meta,
                           verbose = FALSE, heldout.seed = 123)
plot(k_search_output)
k_search_output
```

For this notebook, we will choose K=50 because this value allows to illustrate certain cases in this example. When estimating the topic model, we provide a smoothed version of the day of the year and the politician's party as topic prevalence covariates and do not specify any topic content covariates for now. Already estimating this one model will take some time on the approximately 9,600 documents.

```{r}
stmodel <- stm(documents = stm_input$documents, vocab = stm_input$vocab,
                     K = 50, prevalence =~ party + s(day_of_year),
               data = stm_input$meta, verbose = FALSE, init.type = "Spectral")
```


```{r fig1, fig.height = 5, fig.width = 4}
plot(stmodel)
```

Even with the minimal tuning, the model detects a lot of structure and some interesting topics. For example:

```{r}
cloud(stmodel, topic = 41, scale = c(2,.25))
cloud(stmodel, topic = 35, scale = c(2,.25))
cloud(stmodel, topic = 4, scale = c(2,.25))
cloud(stmodel, topic = 3, scale = c(2,.25))
cloud(stmodel, topic = 42, scale = c(2,.25))
```

One thing to keep in mind when evaluating topic models is that just because something does not show up as a topic, it can still be in the corpus. One (of many) example(s) here is a discussion in the corpus about climate change which does not seem to show up as a clear topic in the current model setting. Thus, while these tools are very helpful for a kind of "automated reading at scale", one will of course miss features of the data. Estimating another model with different K will show some new structures which were previously missed, but other topics can disappear again.

Let us now look into documents which have high shares of these topics and see whether the topic assignment makes sense. This can be done with the function `findThoughts`. We will look at only 2 examples to illustrate the functionality, but of course reading of more articles would be necessary for a careful analysis.

```{r fig1, fig.height = 4, fig.width = 2}
health_care_articles <- findThoughts(stmodel,
                                     texts = df$message[rowSums(posts_dfm)>0],
                                     n = 2, topics = 41)$docs[[1]]
plotQuote(health_care_articles, width = 30,
          main = "Documents containing topic 41")
```

It seems that the health care topic identifies some relevant documents. Yet, looking at the topic about the opioid crisis indicates that the files also contain information about net neutrality.

```{r fig1, fig.height = 5, fig.width = 2}
topic_4_articles <- findThoughts(stmodel,
                                 texts = df$message[rowSums(posts_dfm)>0],
                                 n = 2, topics = 4)$docs[[1]]
plotQuote(topic_4_articles, width = 30, main = "Documents containing topic 4")
```

This emphasises the importance of such an analysis when fitting, evaluating, and calibrating different topic models for a project or paper.

Another great feature of the structural topic model package is that the prevalence covariates allow to see which topics are discussed more by which of the two groups: 

```{r}
# 1:50 refers to topic 1-50
effect_estimates <- estimateEffect(1:50 ~ party + s(day_of_year), stmodel, meta = stm_input$meta)
```


```{r}
plot(effect_estimates, covariate = "party", topics = c(41, 35, 3, 42),
     model = stmodel, method = "difference",
     cov.value1 = "Republican", cov.value2 = "Democrat",
     xlab = "More Democrat ... More Republican", 
     main = "Democrats and Republicans",
     xlim = c(-.1, .1), labeltype = "custom", 
     custom.labels = c("Health care", "Russia investigation",
                       "Immigration and border", "Law enforcement"))
```

Or, through our day of the year covariate, we can plot the discussion of a topic over time, e.g. health care:

```{r}
plot(effect_estimates, "day_of_year", method = "continuous", topics = 41,
     model = stmodel, printlegend = FALSE, xaxt = "n", xlab = "2017",
     main = "Health care topic over time")
monthseq <- seq(from = as.Date("2017-01-01"), to = as.Date("2017-12-01"),
                by = "month")
monthnames <- months(monthseq)
axis(1, at = as.numeric(monthseq) - min(as.numeric(monthseq)),
     labels = monthnames)
```

Lastly, let us illustrate a very helpful interactive tool called LDAvis which allows to explore topic models:

```{r, eval = FALSE}
#install.packages("LDAvis")
toLDAvis(stmodel, docs = stm_input$documents)
```


## 3. Fitting a topic model with topic prevalence and topic content covariates

Next, we fit another structural topic model with both prevalence and topic content covariates. This allows to not just analyse how topic shares within documents vary cross different groups, but also how words in topics vary. The following will take a longer time to run than the previous model because of the additional covariate.


```{r}
stmodel_additional_content_variable <- stm(documents = stm_input$documents,
                                           vocab = stm_input$vocab, K = 50,
                                           prevalence = ~ party + s(date),
                                           content = ~ party,
                                           data = stm_input$meta,
                                           verbose = FALSE,
                                           init.type = "Spectral")
```

Note that this has estimated an new model, so words in topics will be different:

```{r fig1, fig.height = 6, fig.width = 5}
plot(stmodel_additional_content_variable)
```

There is still, however, a similar health care topic:

```{r}
cloud(stmodel_additional_content_variable, topic = 41, scale = c(2,.25))
```

With the help of the new content covariate, we can now analyse how the two parties discuss the same topic differently:

```{r fig1, fig.height = 2, fig.width = 4}
plot(stmodel_additional_content_variable, type = "perspectives", topics = 41)
```

For other topics, however, the results are less reasonable. While the purpose of this notebook is to illustrate a range of functionalities of (structural) topic models with real world data, it also emphasises how important careful model selection and evaluation are in final papers or projects such as http://j.mp/lda-congress-demo. Some exemplary areas and questions that could be explored here would be:

- Aggregation of short documents similarly to BarberÃ¡ et al. (2014)
- Choosing different amounts of K based on careful analysis of topic coherence, reading of documents with regard to their topic allocation, and quantitative model evaluation
- Trying different topic models which might be better suited for short text, see e.g. Bauer et al., Political Behavior, 2017 (the `stm` package e.g. also allows to estimate a SAGE model)
- Could different sets of topic prevalence and topic content covariates be helpful, or could some covariates be dropped for the analysis?
- Is a structural topic model necessary for a certain research question, or could a plain correlated topic model (i.e. no covariates added) be an option?
- Different pre-processing, e.g. dropping more terms with the options in the function `dfm_trim`
- Using a larger corpus with many more than the sample of ~10,000 posts (implies, however, that model estimation would take much longer)
- etc.

For a discussion of model selection and evaluation for structural topic models (e.g. different initialisations, covariates, values of K), see also e.g. Section "3.4. Evaluate: Model selection and search" the package [vignette](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf) or the Section _Model Specification and Selection_ in ``Structural Topic Models for Open-Ended Survey Responses'' by Roberts et al. (2014).


References

- This notebook is based on code from the vignette of the `stm` package https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf. See the vignette also for further functionalities of the package
