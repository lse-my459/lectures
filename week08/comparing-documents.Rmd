---
title: "Comparing documents"
output: html_document
---

# Distance metrics

```{r}
require(quanteda)
require(quanteda.textstats)

```

`textstat_simil()` computes matrices of distances and similarities between documents. It is useful for comparing the feature use across different documents.

Euclidean distance:

```{r}
docs <- c("this is document one", "this is document two")
(doc_dfm <- dfm(docs))
textstat_dist(doc_dfm, method="euclidean")

# let's do the math...
(d1 <- as.numeric(doc_dfm[1,]))
(d2 <- as.numeric(doc_dfm[2,]))

sqrt(sum((d1 - d2)^2))
```

Cosine similarity:

```{r}
textstat_simil(doc_dfm, method="cosine")

# some more math...
sum(d1 * d2) / ( sqrt(sum(d1^2)) *  sqrt(sum(d2^2)) )
```

Note that these two metrics measure the opposite thing: Euclidean distance measures how *different* documents are, whereas cosine similarity measures how *similar* documents are. Of course, it's easy to reverse them; generally, we can just say (1 - distance) = similarity.

Edit distance:

```{r}
textstat_simil(doc_dfm, method="hamann")
```


And here's an example of how we would apply these metrics in practice. Let's say I want to build a recommendation engine for my favorite type of movie.

```{r}
library(readr)
# data source: http://www.cs.cmu.edu/~ark/personas/
movie <- read_csv(unz("movie-plots.csv.zip", "movie-plots.csv"),
                  col_types="cccc")

# now we find a movie i like
scifi <- which(movie$name=="Gravity")
movie[scifi,]

# pre-process the data
mcorp <- corpus(movie, text_field = "plot")
docnames(mcorp) <- docvars(mcorp)$name
mdfm <- dfm(mcorp, verbose=TRUE, remove_punct=TRUE,
            remove_numbers=TRUE, remove=stopwords("english"))

# and I will compute cosine similarity with respect to all other movies
simil <- textstat_simil(dfm_tfidf(mdfm), 
                        selection=scifi, method="cosine")
simil <- simil[order(simil, decreasing=TRUE),]
head(simil, n=5)

# and we can read their plots
movie$plot[movie$name %in% names(simil)[2:6]]

```

# Clustering methods

First we will explore an application of k-means clustering to the plots of recent movies:

```{r}
recent <- corpus_subset(mcorp, release_year>=2010)

mdfm <- dfm(recent, verbose=TRUE, remove_punct=TRUE,
            remove_numbers=TRUE, remove=c(stopwords("english"),
                                          '|', '<', '>', '=', 'accessdate',
                                          'ref', 'cite', 'web'))
cdfm <- dfm_weight(dfm_trim(mdfm, min_docfreq = 5, verbose=TRUE), "prop")

set.seed(1234) # set random seed to ensure replicability
kc <- kmeans(cdfm, centers=4)
table(kc$cluster)
head(docvars(recent)$name[kc$cluster==1])
head(docvars(recent)$name[kc$cluster==2])
head(docvars(recent)$name[kc$cluster==3])
head(docvars(recent)$name[kc$cluster==4])

# Plots with words about movies, rather than words about the movies itself?
head(textstat_keyness(cdfm, target=kc$cluster==1),n=20)
# plots of romantic movies?
head(textstat_keyness(cdfm, target=kc$cluster==2),n=20)
# plots of films about death / murder?
head(textstat_keyness(cdfm, target=kc$cluster==3),n=20)
# plots of "feel-good" movies?
head(textstat_keyness(cdfm, target=kc$cluster==4),n=20)
```

Hierarchical clustering is an alternative approach to group documents. It relies on the matrix of distances between documents and works from the bottom up to create clusters: starting with lowest pairwise distance, then sequentially merges documents into clusters as the distances become larger.

```{r}
library(quanteda.corpora)
pres_dfm <- dfm(corpus_subset(data_corpus_sotu, Date > "1980-01-01"), 
               stem = TRUE, remove_punct = TRUE,
               remove = stopwords("english"))
pres_dfm <- dfm_weight(
  dfm_trim(pres_dfm, min_termfreq = 5, min_docfreq = 3), "prop")

# hierarchical clustering - get distances on normalized dfm
pres_dist_mat <- as.dist(textstat_dist(pres_dfm, method = "euclidean"))

# hiarchical clustering the distance object
pres_cluster <- hclust(pres_dist_mat)

# label with document names
pres_cluster$labels <- docnames(pres_dfm)

# plot dendrogram
plot(pres_cluster)
```




