---
title: "Dictionary methods"
output: html_document
---

## Sentiment detection using dictionary methods

One of the most common applications of dictionary methods is sentiment analysis: using a dictionary of positive and negative words, we compute a sentiment score for each individual document.

Let's apply this technique to tweets by the four leading candidates in the 2016 Presidential primaries. Which candidate was using positive rhetoric most frequently? Which candidate was most negative in their public messages on Twitter?

```{r}
library(quanteda)
tweets <- read.csv('candidate-tweets.csv', stringsAsFactors=F)
```

We will use the positive and negative categories in the augmented General Inquirer dictionary to measure the extent to which these candidates adopted a positive or negative tone during the election campaign.

Note that first you will need to install the `quanteda.dictionaries` package from GitHub

```{r, eval=FALSE}
devtools::install_github("quanteda/quanteda.sentiment")
```

First, we load the dictionary object. Note that we can apply the dictionary object directly (as we will see later), but for now let's learn how to do this if we had a list of positive and negative words on a different file.

```{r}
library(quanteda.sentiment)
data(data_dictionary_geninqposneg)

pos.words <- data_dictionary_geninqposneg[['positive']]
neg.words <- data_dictionary_geninqposneg[['negative']]
# a look at a random sample of positive and negative words
sample(pos.words, 10)
sample(neg.words, 10)
```

As earlier in the course, we will convert our text to a corpus object. Note that here `corpus` takes detects some metadata, which we will use later.

```{r}
twcorpus <- corpus(tweets)
```

Now we're ready to run the sentiment analysis! First we will construct a dictionary object.

```{r}
sent_dictionary <- dictionary(list(positive = pos.words,
                          negative = neg.words))
```

And now we apply it to the corpus in order to count the number of words that appear in each category

```{r}
toks <- tokens(twcorpus)
dfm <- dfm(toks)
sent <- dfm_lookup(dfm, sent_dictionary)
```

We can then extract the score and add it to the data frame as a new variable

```{r}
tweets$score <- as.numeric(sent[,1]) - as.numeric(sent[,2])
```

And now start answering some descriptive questions...

```{r}
# what is the average sentiment score?
mean(tweets$score)
# what is the most positive and most negative tweet?
tweets[which.max(tweets$score),]
tweets[which.min(tweets$score),]
# what is the proportion of positive, neutral, and negative tweets?
tweets$sentiment <- "neutral"
tweets$sentiment[tweets$score<0] <- "negative"
tweets$sentiment[tweets$score>0] <- "positive"
table(tweets$sentiment)
```

We can also compute it at the candidate level by taking the average of the sentiment scores:

```{r}
# loop over candidates
candidates <- c("realDonaldTrump", "HillaryClinton", "tedcruz", "BernieSanders")

for (cand in candidates){
  message(cand, " -- average sentiment: ",
      round(mean(tweets$score[tweets$screen_name==cand]), 4)
    )
}
```

But what happens if we now run the analysis excluding a single word?

```{r}
pos.words <- pos.words[-which(pos.words=="great")]

sent_dictionary <- dictionary(list(positive = pos.words,
                          negative = neg.words))
toks <- tokens(twcorpus)
sent <- dfm(toks, dictionary = sent_dictionary)
tweets$score <- as.numeric(sent[,1]) - as.numeric(sent[,2])

for (cand in candidates){
  message(cand, " -- average sentiment: ",
      round(mean(tweets$score[tweets$screen_name==cand]), 4)
    )
}

```

How would we normalize by text length? (Maybe not necessary here given that tweets have roughly the same length.)

```{r}
# collapse by account into 4 documents
toks <- tokens(twcorpus)
twdfm <- dfm(toks)
twdfm <- dfm_group(twdfm, groups = screen_name)
twdfm

# turn word counts into proportions
twdfm[1:4, 1:10]
twdfm <- dfm_weight(twdfm, scheme="prop")
twdfm[1:4, 1:10]

# Apply dictionary using `dfm_lookup()` function:
sent <- dfm_lookup(twdfm, dictionary = sent_dictionary)
sent
(sent[,1]-sent[,2])

```

Finally, let's apply a different dictionary so that we can practice with dictionaries in different formats:

```{r}
data(data_dictionary_MFD)
# dictionary keys
names(data_dictionary_MFD)
# looking at words within first key
data_dictionary_MFD$care.virtue[1:10]

# applying dictionary
# 1) collapse by account
toks <- tokens(twcorpus)
twdfm <- dfm(toks)
twdfm <- dfm_group(twdfm, groups = screen_name)
# 2) turn words into proportions
twdfm <- dfm_weight(twdfm, scheme="prop")
# 3) apply dictionary
moral <- dfm_lookup(twdfm, dictionary = data_dictionary_MFD)

# are liberals more sensitive to care and virtue?
dfm_sort(moral[,'care.virtue']*100, margin='documents')
dfm_sort(moral[,'fairness.virtue']*100, margin='documents')

# are conservatives more sensitive to sanctity and authority?
dfm_sort(moral[,'sanctity.virtue']*100, margin='documents')
dfm_sort(moral[,'authority.virtue']*100, margin='documents')

```

## Identifying most unique features of documents

_Keyness_ is a measure of to what extent some features are specific to a (group of) document in comparison to the rest of the corpus, taking into account that some features may be too rare.

```{r}
library(quanteda.textplots)
library(quanteda.textstats)
toks <- tokens(twcorpus)
twdfm <- dfm(toks)
twdfm <- dfm_group(twdfm, groups=c(screen_name))

# Donald Trump
head(textstat_keyness(twdfm, target="realDonaldTrump",
                      measure="chi2"), n=20)
textstat_keyness(twdfm, target="realDonaldTrump",
                      measure="chi2") %>% textplot_keyness()

# Hillary Clinton
head(textstat_keyness(twdfm, target="HillaryClinton",
                      measure="chi2"), n=20)
textstat_keyness(twdfm, target="HillaryClinton",
                      measure="chi2") %>% textplot_keyness()

# Ted Cruz
head(textstat_keyness(twdfm, target="tedcruz",
                      measure="chi2"), n=20)
textstat_keyness(twdfm, target="tedcruz",
                      measure="chi2") %>% textplot_keyness()

# Bernie Sanders
head(textstat_keyness(twdfm, target="BernieSanders",
                      measure="chi2"), n=20)
textstat_keyness(twdfm, target="BernieSanders",
                      measure="chi2") %>% textplot_keyness()

```

If we have other metadata in our corpus, we can also use it to identify words whose usage varies over time:

```{r}
trump <- corpus_subset(twcorpus, screen_name=="realDonaldTrump")
twdfm <- dfm(trump, remove_punct=TRUE,
             remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)

textstat_keyness(twdfm, target=docvars(twdfm)$datetime>"2016-05", 
                      measure="chi2") %>% textplot_keyness()

```

